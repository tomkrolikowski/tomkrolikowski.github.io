<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Tom Krolikowski</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <link href="http://localhost:4000/"/>
 <updated>2023-10-04T10:19:50-04:00</updated>
 <id>http://localhost:4000</id>
 <author>
   <name></name>
   <email></email>
 </author>

 
 <entry>
   <title>BrushLens: Hardware Interaction Proxies for Accessible Touchscreen Interface Actuation</title>
   <link href="http://localhost:4000/2023/10/04/brushlens/"/>
   <updated>2023-10-04T00:00:00-04:00</updated>
   <id>http://localhost:4000/2023/10/04/brushlens</id>
   <content type="html">
</content>
 </entry>
 
 <entry>
   <title>SNACBot</title>
   <link href="http://localhost:4000/2022/04/21/snacbot/"/>
   <updated>2022-04-21T00:00:00-04:00</updated>
   <id>http://localhost:4000/2022/04/21/snacbot</id>
   <content type="html">&lt;p&gt;Our group created the 5 degree of freedom robotic arm using dynamixel servos and 3D printing components designed through fusion 360. In order to accomplish object detection, we used transfer learning on a pretrained YOLOv5 nano SSD model. We also implemented and tested multiple segmentation methods for determining the gripper pose. All of the software was designed and implemented within ROS (Robotic Operating System) as a middleware.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/img/training_graphs.png&quot; /&gt;
Training plots obtained from training the YOLOv5 neural network on the custom dataset.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/mAP-0.5.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The area under the precision-recall curve (AUPRC) graph under an IoU threshold of 0.5 for each class in the dataset and their mean.&lt;/p&gt;

&lt;h2 id=&quot;segmentation-results&quot;&gt;Segmentation Results&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/img/Test_Cases_fig.png&quot; /&gt;
Three test cases. From left to right, RGB image, aligned depth map, k-means on Depth map with 3 clusters, k-means on RGB and depth channels with 4 clusters, least squares fitting of a plane.&lt;/p&gt;

&lt;h2 id=&quot;state_machine&quot;&gt;state_machine&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/img/state_machine.png&quot; /&gt;
High level state machine of SNACBotâ€™s workflow&lt;/p&gt;

&lt;div class=&quot;foot-icon&quot;&gt; 
&lt;a href=&quot;https://github.com/hvak/SNACBot&quot;&gt;&lt;span class=&quot;fa fa-github fa-2x white&quot; style=&quot;text-align:center; color:rgb(0, 0, 0);&quot;&gt;&lt;/span&gt;&lt;a&gt;
&lt;/a&gt;&lt;/a&gt;&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Semantic Segmentation Depth Estimation</title>
   <link href="http://localhost:4000/2021/04/21/depth_estimation/"/>
   <updated>2021-04-21T00:00:00-04:00</updated>
   <id>http://localhost:4000/2021/04/21/depth_estimation</id>
   <content type="html">&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;p&gt;For this project, our group utilized transfer learning using resnet-18 and Unet as pretrained models. We trained each model using the Dense Indoor and Outdoor Depth (DIODE) dataset. Although, to increase the training time, we resized the images in the dataset from 768x1024 to 384x512. After training, our group created a small python script to simulate a lense focusing on a particular depth.&lt;/p&gt;

&lt;h2 id=&quot;indoor-results&quot;&gt;Indoor Results&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/img/indoorresults.png&quot; /&gt;
From left to right, RGB image, ground-truth depth map, ResNet18 predicted depth map, and UNet predicted depth examples from the DIODE indoor data set.&lt;/p&gt;

&lt;h2 id=&quot;outdoor-results&quot;&gt;Outdoor Results&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/img/outdoorresults.png&quot; /&gt;
From left to right, RGB image, ground-truth depth map, ResNet18 predicted depth map, and UNet predicted depth examples from the DIODE outdoor data set.&lt;/p&gt;

&lt;div class=&quot;foot-icon&quot;&gt; 
&lt;a href=&quot;https://github.com/lanbas/442-depth-estimation&quot;&gt;&lt;span class=&quot;fa fa-github fa-2x&quot; style=&quot;color:rgb(0, 0, 0);&quot;&gt;&lt;/span&gt;&lt;a&gt;
&lt;/a&gt;&lt;/a&gt;&lt;/div&gt;
</content>
 </entry>
 

</feed>
