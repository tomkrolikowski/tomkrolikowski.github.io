---
layout: post
title: SNACBot
type: project
image: snacbot_header.jpg
video: https://www.youtube.com/embed/f-vsUyTJYIk
sections:
  intro: SNACBot utilizes a robot arm to deliver food to a person autonomously. The robot uses computer vision to locate food on a surface, and navigate the arm toward it to pick it up. SNACBot will also detect a user with an open mouth and deliver the food to their mouth safely. This allows disabled users to enjoy a meal on their own without the assistance of a caregiver.
github: https://github.com/hvak/SNACBot
---

Our group created the 5 degree of freedom robotic arm using dynamixel servos and 3D printing components designed through fusion 360. In order to accomplish object detection, we used transfer learning on a pretrained YOLOv5 nano SSD model. We also implemented and tested multiple segmentation methods for determining the gripper pose. All of the software was designed and implemented within ROS (Robotic Operating System) as a middleware. 

## Results
<img src="/assets/img/training_graphs.png">
Training plots obtained from training the YOLOv5 neural network on the custom dataset.


<img src="/assets/img/mAP-0.5.png">

The area under the precision-recall curve (AUPRC) graph under an IoU threshold of 0.5 for each class in the dataset and their mean.

## Segmentation Results
<img src="/assets/img/Test_Cases_fig.png">
Three test cases. From left to right, RGB image, aligned depth map, k-means on Depth map with 3 clusters, k-means on RGB and depth channels with 4 clusters, least squares fitting of a plane.

## state_machine
<img src="/assets/img/state_machine.png">
High level state machine of SNACBot's workflow

<div class="foot-icon"> 
<a href="{{page.github}}"><span class="fa fa-github fa-2x white" style="text-align:center; color:rgb(0, 0, 0);"></span><a>
